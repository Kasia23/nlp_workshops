{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Syjamska sieć neuronowa"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ten notebook przedstawia użycie syjamskiej sieci neuronowej na danych tekstowych.\n",
    "Sieć oparta jest na dwóch modelach blstm o takiej samej architekturze i współdzielonych wagach.\n",
    "\n",
    "Wykorzystamy dane z kaggle dotyczące pytań na Quora i podobieństwa między nimi:\n",
    "https://www.kaggle.com/c/quora-question-pairs\n",
    "\n",
    "Kod podzielony jest na notebooka i skrypty pythonowe, z których zaczytujemy potrzebne funkcje."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from sklearn.model_selection import train_test_split\n",
    "from keras.callbacks import TensorBoard\n",
    "from string import punctuation\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import time\n",
    "import datetime\n",
    "import pickle\n",
    "import nbimporter\n",
    "from utils.preprocessing_utils import clear_offers, prepare_representation\n",
    "from utils.model_utils import prepare_embedding_matrix, one_or_zero, build_model_blstm, exponent_neg_manhattan_distance, calculate_preds_binary, model_statistics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Definiujemy timestamp, który umożliwi nam wersjonowanie danych, modelu i tokenizera przy zapisywaniu."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "start_time = time.time()\n",
    "now = time.strftime(\"%Y%m%d-%H%M%S\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Wczytanie danych"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Opis danych ze strony:\n",
    "\n",
    "Over 100 million people visit Quora every month, so it's no surprise that many people ask similarly worded questions. Multiple questions with the same intent can cause seekers to spend more time finding the best answer to their question, and make writers feel they need to answer multiple versions of the same question. Quora values canonical questions because they provide a better experience to active seekers and writers, and offer more value to both of these groups in the long term.\n",
    "\n",
    "The goal of this competition is to predict which of the provided pairs of questions contain two questions with the same meaning. \n",
    "\n",
    "Data fields:\n",
    "id - the id of a training set question pair\n",
    "qid1, qid2 - unique ids of each question (only available in train.csv)\n",
    "question1, question2 - the full text of each question\n",
    "is_duplicate - the target variable, set to 1 if question1 and question2 have essentially the same meaning, and 0 otherwise."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv(\"src/Siamese_workshops_quora.csv\", index_col=\"id\", nrows=10000)\n",
    "data = data[data['question1'].apply(lambda x: isinstance(x,str))]\n",
    "data = data[data['question2'].apply(lambda x: isinstance(x,str))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>qid1</th>\n",
       "      <th>qid2</th>\n",
       "      <th>question1</th>\n",
       "      <th>question2</th>\n",
       "      <th>is_duplicate</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>id</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>What is the step by step guide to invest in sh...</td>\n",
       "      <td>What is the step by step guide to invest in sh...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>What is the story of Kohinoor (Koh-i-Noor) Dia...</td>\n",
       "      <td>What would happen if the Indian government sto...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>5</td>\n",
       "      <td>6</td>\n",
       "      <td>How can I increase the speed of my internet co...</td>\n",
       "      <td>How can Internet speed be increased by hacking...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>7</td>\n",
       "      <td>8</td>\n",
       "      <td>Why am I mentally very lonely? How can I solve...</td>\n",
       "      <td>Find the remainder when [math]23^{24}[/math] i...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>9</td>\n",
       "      <td>10</td>\n",
       "      <td>Which one dissolve in water quikly sugar, salt...</td>\n",
       "      <td>Which fish would survive in salt water?</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    qid1  qid2                                          question1  \\\n",
       "id                                                                  \n",
       "0      1     2  What is the step by step guide to invest in sh...   \n",
       "1      3     4  What is the story of Kohinoor (Koh-i-Noor) Dia...   \n",
       "2      5     6  How can I increase the speed of my internet co...   \n",
       "3      7     8  Why am I mentally very lonely? How can I solve...   \n",
       "4      9    10  Which one dissolve in water quikly sugar, salt...   \n",
       "\n",
       "                                            question2  is_duplicate  \n",
       "id                                                                   \n",
       "0   What is the step by step guide to invest in sh...             0  \n",
       "1   What would happen if the Indian government sto...             0  \n",
       "2   How can Internet speed be increased by hacking...             0  \n",
       "3   Find the remainder when [math]23^{24}[/math] i...             0  \n",
       "4             Which fish would survive in salt water?             0  "
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>qid1</th>\n",
       "      <th>qid2</th>\n",
       "      <th>question1</th>\n",
       "      <th>question2</th>\n",
       "      <th>is_duplicate</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>id</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>9995</th>\n",
       "      <td>19404</td>\n",
       "      <td>19405</td>\n",
       "      <td>How would you order these four cities (Bangalo...</td>\n",
       "      <td>What is the cost of living in Europe and the U...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9996</th>\n",
       "      <td>19406</td>\n",
       "      <td>19407</td>\n",
       "      <td>Stphen william hawking?</td>\n",
       "      <td>What are the differences between SM, YG and JY...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9997</th>\n",
       "      <td>19408</td>\n",
       "      <td>19409</td>\n",
       "      <td>Mathematical Puzzles: What is () + () + () = 3...</td>\n",
       "      <td>What are the steps to solve this equation: [ma...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9998</th>\n",
       "      <td>19410</td>\n",
       "      <td>19411</td>\n",
       "      <td>Is IMS noida good for BCA?</td>\n",
       "      <td>How good is IMS Noida for studying BCA?</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9999</th>\n",
       "      <td>19412</td>\n",
       "      <td>19413</td>\n",
       "      <td>What are the most respected and informative te...</td>\n",
       "      <td>What are Caltech's required and recommended te...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       qid1   qid2                                          question1  \\\n",
       "id                                                                      \n",
       "9995  19404  19405  How would you order these four cities (Bangalo...   \n",
       "9996  19406  19407                            Stphen william hawking?   \n",
       "9997  19408  19409  Mathematical Puzzles: What is () + () + () = 3...   \n",
       "9998  19410  19411                         Is IMS noida good for BCA?   \n",
       "9999  19412  19413  What are the most respected and informative te...   \n",
       "\n",
       "                                              question2  is_duplicate  \n",
       "id                                                                     \n",
       "9995  What is the cost of living in Europe and the U...             0  \n",
       "9996  What are the differences between SM, YG and JY...             0  \n",
       "9997  What are the steps to solve this equation: [ma...             0  \n",
       "9998            How good is IMS Noida for studying BCA?             1  \n",
       "9999  What are Caltech's required and recommended te...             0  "
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3711, 5)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data[data.is_duplicate==1].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(6289, 5)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data[data.is_duplicate==0].shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Przygotowanie tekstu"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Czyścimy dane z interpunkcji i lemmatyzujemy tekst. Nie usuwamy stopwordsów, ponieważ ich brak mógłby znacząco zmienić sens pytania.\n",
    "Używamy funkcji clear_text, która przyjmuje argumenty: dane, znaki do usunięcia, czy usuwać stopwordsy i czy lematyzować tekst.\n",
    "Następnie wykonuje szereg zadanych operacji i zwraca pytania podzielone na oczyszczone słowa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019-04-10 15:50:34.461892 Oczyszczenie danych - SUKCES\n",
      "2019-04-10 15:50:38.236430 Lemmatyzacja - SUKCES\n",
      "2019-04-10 15:50:38.652433 Oczyszczenie danych - SUKCES\n",
      "2019-04-10 15:50:39.403429 Lemmatyzacja - SUKCES\n"
     ]
    }
   ],
   "source": [
    "for question in ['question1', 'question2']:\n",
    "    strip_chars = punctuation + '„”–'\n",
    "    data[question + '_cleared'] = clear_text(data[question], strip_chars, is_remove_stopwords=False, is_lemmatize=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>qid1</th>\n",
       "      <th>qid2</th>\n",
       "      <th>question1</th>\n",
       "      <th>question2</th>\n",
       "      <th>is_duplicate</th>\n",
       "      <th>question1_cleared</th>\n",
       "      <th>question2_cleared</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>id</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>9995</th>\n",
       "      <td>19404</td>\n",
       "      <td>19405</td>\n",
       "      <td>How would you order these four cities (Bangalo...</td>\n",
       "      <td>What is the cost of living in Europe and the U...</td>\n",
       "      <td>0</td>\n",
       "      <td>[how, would, you, order, these, four, city, ba...</td>\n",
       "      <td>[what, is, the, cost, of, living, in, europe, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9996</th>\n",
       "      <td>19406</td>\n",
       "      <td>19407</td>\n",
       "      <td>Stphen william hawking?</td>\n",
       "      <td>What are the differences between SM, YG and JY...</td>\n",
       "      <td>0</td>\n",
       "      <td>[stphen, william, hawking]</td>\n",
       "      <td>[what, are, the, difference, between, sm, yg, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9997</th>\n",
       "      <td>19408</td>\n",
       "      <td>19409</td>\n",
       "      <td>Mathematical Puzzles: What is () + () + () = 3...</td>\n",
       "      <td>What are the steps to solve this equation: [ma...</td>\n",
       "      <td>0</td>\n",
       "      <td>[mathematical, puzzle, what, is, 30, using, 1,...</td>\n",
       "      <td>[what, are, the, step, to, solve, this, equati...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9998</th>\n",
       "      <td>19410</td>\n",
       "      <td>19411</td>\n",
       "      <td>Is IMS noida good for BCA?</td>\n",
       "      <td>How good is IMS Noida for studying BCA?</td>\n",
       "      <td>1</td>\n",
       "      <td>[is, ims, noida, good, for, bca]</td>\n",
       "      <td>[how, good, is, ims, noida, for, studying, bca]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9999</th>\n",
       "      <td>19412</td>\n",
       "      <td>19413</td>\n",
       "      <td>What are the most respected and informative te...</td>\n",
       "      <td>What are Caltech's required and recommended te...</td>\n",
       "      <td>0</td>\n",
       "      <td>[what, are, the, most, respected, and, informa...</td>\n",
       "      <td>[what, are, caltech's, required, and, recommen...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       qid1   qid2                                          question1  \\\n",
       "id                                                                      \n",
       "9995  19404  19405  How would you order these four cities (Bangalo...   \n",
       "9996  19406  19407                            Stphen william hawking?   \n",
       "9997  19408  19409  Mathematical Puzzles: What is () + () + () = 3...   \n",
       "9998  19410  19411                         Is IMS noida good for BCA?   \n",
       "9999  19412  19413  What are the most respected and informative te...   \n",
       "\n",
       "                                              question2  is_duplicate  \\\n",
       "id                                                                      \n",
       "9995  What is the cost of living in Europe and the U...             0   \n",
       "9996  What are the differences between SM, YG and JY...             0   \n",
       "9997  What are the steps to solve this equation: [ma...             0   \n",
       "9998            How good is IMS Noida for studying BCA?             1   \n",
       "9999  What are Caltech's required and recommended te...             0   \n",
       "\n",
       "                                      question1_cleared  \\\n",
       "id                                                        \n",
       "9995  [how, would, you, order, these, four, city, ba...   \n",
       "9996                         [stphen, william, hawking]   \n",
       "9997  [mathematical, puzzle, what, is, 30, using, 1,...   \n",
       "9998                   [is, ims, noida, good, for, bca]   \n",
       "9999  [what, are, the, most, respected, and, informa...   \n",
       "\n",
       "                                      question2_cleared  \n",
       "id                                                       \n",
       "9995  [what, is, the, cost, of, living, in, europe, ...  \n",
       "9996  [what, are, the, difference, between, sm, yg, ...  \n",
       "9997  [what, are, the, step, to, solve, this, equati...  \n",
       "9998    [how, good, is, ims, noida, for, studying, bca]  \n",
       "9999  [what, are, caltech's, required, and, recommen...  "
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.tail()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tworzenie i zapisywanie tokenizera"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tworzymy i zapisujemy tokenizer. Przyda nam się on gdy będziemy chcieli ponownie użyć modelu i przygotować do niego dowolny zbiór danych\n",
    "Używamy funkcji prepare_representation, która przyjmuje dane tekstowe z obydwu pytań (korpus danych), tworzy tokenizer i zwraca zarówno tokenizer jak i ztokenizowaną treść pytań. \n",
    "\n",
    "Argument oov_token: opcjonalny argument, który zostanie dodany do indeksu słów i użyty za każdym razem kiedy pojawi się słowo, które nie było uwzględnione w tokenizerze."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer, stacked_representation = prepare_representation(\n",
    "    pd.concat([data['question1_cleared'], data['question2_cleared']], axis=0), 'unk')\n",
    "data['question1_tokens'], data['question2_tokens'] = np.array_split(stacked_representation, 2)\n",
    "with open(f\"results/{now}_tokenizer_warsztaty.pickle\", 'wb') as handle:\n",
    "    pickle.dump(tokenizer, handle, protocol=pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>qid1</th>\n",
       "      <th>qid2</th>\n",
       "      <th>question1</th>\n",
       "      <th>question2</th>\n",
       "      <th>is_duplicate</th>\n",
       "      <th>question1_cleared</th>\n",
       "      <th>question2_cleared</th>\n",
       "      <th>question1_tokens</th>\n",
       "      <th>question2_tokens</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>id</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>9995</th>\n",
       "      <td>19404</td>\n",
       "      <td>19405</td>\n",
       "      <td>How would you order these four cities (Bangalo...</td>\n",
       "      <td>What is the cost of living in Europe and the U...</td>\n",
       "      <td>0</td>\n",
       "      <td>[how, would, you, order, these, four, city, ba...</td>\n",
       "      <td>[what, is, the, cost, of, living, in, europe, ...</td>\n",
       "      <td>[7, 46, 16, 515, 352, 1556, 321, 488, 1018, 65...</td>\n",
       "      <td>[3, 4, 2, 277, 11, 426, 9, 1104, 13, 2, 85, 59...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9996</th>\n",
       "      <td>19406</td>\n",
       "      <td>19407</td>\n",
       "      <td>Stphen william hawking?</td>\n",
       "      <td>What are the differences between SM, YG and JY...</td>\n",
       "      <td>0</td>\n",
       "      <td>[stphen, william, hawking]</td>\n",
       "      <td>[what, are, the, difference, between, sm, yg, ...</td>\n",
       "      <td>[11146, 11147, 11148]</td>\n",
       "      <td>[3, 12, 2, 66, 50, 2017, 13747, 13, 13748, 611...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9997</th>\n",
       "      <td>19408</td>\n",
       "      <td>19409</td>\n",
       "      <td>Mathematical Puzzles: What is () + () + () = 3...</td>\n",
       "      <td>What are the steps to solve this equation: [ma...</td>\n",
       "      <td>0</td>\n",
       "      <td>[mathematical, puzzle, what, is, 30, using, 1,...</td>\n",
       "      <td>[what, are, the, step, to, solve, this, equati...</td>\n",
       "      <td>[2985, 2738, 3, 4, 753, 177, 109, 144, 188, 31...</td>\n",
       "      <td>[3, 12, 2, 649, 8, 791, 69, 1308, 192, 1153, 2...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9998</th>\n",
       "      <td>19410</td>\n",
       "      <td>19411</td>\n",
       "      <td>Is IMS noida good for BCA?</td>\n",
       "      <td>How good is IMS Noida for studying BCA?</td>\n",
       "      <td>1</td>\n",
       "      <td>[is, ims, noida, good, for, bca]</td>\n",
       "      <td>[how, good, is, ims, noida, for, studying, bca]</td>\n",
       "      <td>[4, 7873, 5462, 40, 15, 7874]</td>\n",
       "      <td>[7, 40, 4, 7873, 5462, 15, 845, 7874]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9999</th>\n",
       "      <td>19412</td>\n",
       "      <td>19413</td>\n",
       "      <td>What are the most respected and informative te...</td>\n",
       "      <td>What are Caltech's required and recommended te...</td>\n",
       "      <td>0</td>\n",
       "      <td>[what, are, the, most, respected, and, informa...</td>\n",
       "      <td>[what, are, caltech's, required, and, recommen...</td>\n",
       "      <td>[3, 12, 2, 56, 3418, 13, 11149, 2184, 15, 845,...</td>\n",
       "      <td>[3, 12, 13751, 579, 13, 1858, 2184, 15, 2158, ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       qid1   qid2                                          question1  \\\n",
       "id                                                                      \n",
       "9995  19404  19405  How would you order these four cities (Bangalo...   \n",
       "9996  19406  19407                            Stphen william hawking?   \n",
       "9997  19408  19409  Mathematical Puzzles: What is () + () + () = 3...   \n",
       "9998  19410  19411                         Is IMS noida good for BCA?   \n",
       "9999  19412  19413  What are the most respected and informative te...   \n",
       "\n",
       "                                              question2  is_duplicate  \\\n",
       "id                                                                      \n",
       "9995  What is the cost of living in Europe and the U...             0   \n",
       "9996  What are the differences between SM, YG and JY...             0   \n",
       "9997  What are the steps to solve this equation: [ma...             0   \n",
       "9998            How good is IMS Noida for studying BCA?             1   \n",
       "9999  What are Caltech's required and recommended te...             0   \n",
       "\n",
       "                                      question1_cleared  \\\n",
       "id                                                        \n",
       "9995  [how, would, you, order, these, four, city, ba...   \n",
       "9996                         [stphen, william, hawking]   \n",
       "9997  [mathematical, puzzle, what, is, 30, using, 1,...   \n",
       "9998                   [is, ims, noida, good, for, bca]   \n",
       "9999  [what, are, the, most, respected, and, informa...   \n",
       "\n",
       "                                      question2_cleared  \\\n",
       "id                                                        \n",
       "9995  [what, is, the, cost, of, living, in, europe, ...   \n",
       "9996  [what, are, the, difference, between, sm, yg, ...   \n",
       "9997  [what, are, the, step, to, solve, this, equati...   \n",
       "9998    [how, good, is, ims, noida, for, studying, bca]   \n",
       "9999  [what, are, caltech's, required, and, recommen...   \n",
       "\n",
       "                                       question1_tokens  \\\n",
       "id                                                        \n",
       "9995  [7, 46, 16, 515, 352, 1556, 321, 488, 1018, 65...   \n",
       "9996                              [11146, 11147, 11148]   \n",
       "9997  [2985, 2738, 3, 4, 753, 177, 109, 144, 188, 31...   \n",
       "9998                      [4, 7873, 5462, 40, 15, 7874]   \n",
       "9999  [3, 12, 2, 56, 3418, 13, 11149, 2184, 15, 845,...   \n",
       "\n",
       "                                       question2_tokens  \n",
       "id                                                       \n",
       "9995  [3, 4, 2, 277, 11, 426, 9, 1104, 13, 2, 85, 59...  \n",
       "9996  [3, 12, 2, 66, 50, 2017, 13747, 13, 13748, 611...  \n",
       "9997  [3, 12, 2, 649, 8, 791, 69, 1308, 192, 1153, 2...  \n",
       "9998              [7, 40, 4, 7873, 5462, 15, 845, 7874]  \n",
       "9999  [3, 12, 13751, 579, 13, 1858, 2184, 15, 2158, ...  "
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.tail()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ustalenie parametrów modelu i podzielenie danych"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "opisac parametry modelu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_parameters = {\n",
    "        'emb_len': 32,\n",
    "        'lstm_units': 10,\n",
    "        'max_seq_len': 150,\n",
    "        'offer_rep_dim': 64,\n",
    "        'batch_size': 64,\n",
    "        'maxlen': 40,\n",
    "        'distance': 'manhattan',\n",
    "        'optimizer': 'adam',\n",
    "        'loss': 'bin'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = data.set_index(['qid1', 'qid2'])\n",
    "\n",
    "y = data['is_duplicate'].astype(np.int64).apply(one_or_zero, args=(1,))\n",
    "\n",
    "\n",
    "Y_train, Y_validation, X_train, X_validation = train_test_split(y, data.drop([\"is_duplicate\"], axis=1), test_size=0.2)\n",
    "\n",
    "X_train = X_train.drop([\"question1\",\"question2\", \"question1_cleared\", \"question2_cleared\"], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th>question1_tokens</th>\n",
       "      <th>question2_tokens</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>qid1</th>\n",
       "      <th>qid2</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>19273</th>\n",
       "      <th>19274</th>\n",
       "      <td>[7, 14, 6, 24, 5, 40, 225, 260]</td>\n",
       "      <td>[3, 4, 5, 40, 42, 8, 98, 5, 40, 225, 667]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12214</th>\n",
       "      <th>3938</th>\n",
       "      <td>[7, 14, 6, 180, 19, 2274, 11, 122, 241]</td>\n",
       "      <td>[3, 31, 6, 10, 8, 180, 19, 122]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3075</th>\n",
       "      <th>3076</th>\n",
       "      <td>[64, 10, 1059, 1311, 1829, 9, 2, 752, 17, 10, ...</td>\n",
       "      <td>[3, 211, 1059, 1311, 9, 2, 752]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5120</th>\n",
       "      <th>5121</th>\n",
       "      <td>[17, 10, 41, 70, 30, 65, 12, 110, 99, 1142, 13...</td>\n",
       "      <td>[17, 4, 18, 30, 41, 70, 30, 65, 12, 110, 99, 1...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10306</th>\n",
       "      <th>10307</th>\n",
       "      <td>[26, 4, 2, 56, 3655, 97, 15, 16]</td>\n",
       "      <td>[26, 4, 2, 56, 5454, 97, 96]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              question1_tokens  \\\n",
       "qid1  qid2                                                       \n",
       "19273 19274                    [7, 14, 6, 24, 5, 40, 225, 260]   \n",
       "12214 3938             [7, 14, 6, 180, 19, 2274, 11, 122, 241]   \n",
       "3075  3076   [64, 10, 1059, 1311, 1829, 9, 2, 752, 17, 10, ...   \n",
       "5120  5121   [17, 10, 41, 70, 30, 65, 12, 110, 99, 1142, 13...   \n",
       "10306 10307                   [26, 4, 2, 56, 3655, 97, 15, 16]   \n",
       "\n",
       "                                              question2_tokens  \n",
       "qid1  qid2                                                      \n",
       "19273 19274          [3, 4, 5, 40, 42, 8, 98, 5, 40, 225, 667]  \n",
       "12214 3938                     [3, 31, 6, 10, 8, 180, 19, 122]  \n",
       "3075  3076                     [3, 211, 1059, 1311, 9, 2, 752]  \n",
       "5120  5121   [17, 4, 18, 30, 41, 70, 30, 65, 12, 110, 99, 1...  \n",
       "10306 10307                       [26, 4, 2, 56, 5454, 97, 96]  "
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_dataset = [pad_sequences(X_train['question1_tokens'], maxlen=model_parameters['maxlen']),\n",
    "                   pad_sequences(X_train['question2_tokens'], maxlen=model_parameters['maxlen'])]\n",
    "X_val_dataset = [pad_sequences(X_validation['question1_tokens'], maxlen=model_parameters['maxlen']),\n",
    "                 pad_sequences(X_validation['question2_tokens'], maxlen=model_parameters['maxlen'])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[array([[   0,    0,    0, ...,   40,  225,  260],\n",
       "        [   0,    0,    0, ...,   11,  122,  241],\n",
       "        [   0,    0,    0, ...,   10,   65,  207],\n",
       "        ...,\n",
       "        [   0,    0,    0, ..., 1958,   58, 6516],\n",
       "        [   0,    0,    0, ...,  176,   29,   69],\n",
       "        [   0,    0,    0, ..., 5077, 5078,  789]]),\n",
       " array([[   0,    0,    0, ...,   40,  225,  667],\n",
       "        [   0,    0,    0, ...,  180,   19,  122],\n",
       "        [   0,    0,    0, ...,    9,    2,  752],\n",
       "        ...,\n",
       "        [   0,    0,    0, ...,    3,    4, 6516],\n",
       "        [   0,    0,    0, ...,    6,   10,  176],\n",
       "        [   0,    0,    0, ...,   13, 5078,  789]])]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train_dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There two possible embedddings for this model: fb_emb or train your own embeddings.\n",
    "Opisać funkcje i parametry"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_parameters.update({'nb_tokens': len(tokenizer.index_word) + 1})\n",
    "\n",
    "embedding_matrix, embeddings_index, is_trainable = prepare_embedding_matrix(model_parameters, tokenizer.word_index,\n",
    "                                                                            'src/wiki.en.vec.csv', 'own', nrows=None)\n",
    "model_parameters.update({'is_trainable': is_trainable})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Building and training model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Opisać funkcję"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_1 (InputLayer)            (None, 40)           0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_2 (InputLayer)            (None, 40)           0                                            \n",
      "__________________________________________________________________________________________________\n",
      "embedding_1 (Embedding)         (None, 40, 32)       440064      input_1[0][0]                    \n",
      "                                                                 input_2[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "bidirectional_1 (Bidirectional) (None, 40, 20)       3440        embedding_1[0][0]                \n",
      "                                                                 embedding_1[1][0]                \n",
      "__________________________________________________________________________________________________\n",
      "dropout_1 (Dropout)             (None, 40, 20)       0           bidirectional_1[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "dropout_2 (Dropout)             (None, 40, 20)       0           bidirectional_1[1][0]            \n",
      "__________________________________________________________________________________________________\n",
      "dense_1 (Dense)                 (None, 40, 64)       1344        dropout_1[0][0]                  \n",
      "                                                                 dropout_2[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "flatten_1 (Flatten)             (None, 2560)         0           dense_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "flatten_2 (Flatten)             (None, 2560)         0           dense_1[1][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "lambda_1 (Lambda)               (None, 1)            0           flatten_1[0][0]                  \n",
      "                                                                 flatten_2[0][0]                  \n",
      "==================================================================================================\n",
      "Total params: 444,848\n",
      "Trainable params: 444,848\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "model = build_model_blstm(model_parameters, embedding_matrix)\n",
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "ones_share = Y_train.sum() / Y_train.shape[0]\n",
    "\n",
    "ones_weight = (1 - ones_share) / ones_share"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 7200 samples, validate on 800 samples\n",
      "Epoch 1/1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5440/7200 [=====================>........] - ETA: 6:35 - loss: 2.7592 - acc: 0.6406 - mean_absolute_error: 0.3622 - cosine_proximity: -0.359 - ETA: 3:23 - loss: 2.5071 - acc: 0.6484 - mean_absolute_error: 0.3537 - cosine_proximity: -0.351 - ETA: 2:18 - loss: 2.4314 - acc: 0.6406 - mean_absolute_error: 0.3628 - cosine_proximity: -0.359 - ETA: 1:46 - loss: 2.4420 - acc: 0.6211 - mean_absolute_error: 0.3819 - cosine_proximity: -0.378 - ETA: 1:26 - loss: 2.2893 - acc: 0.6250 - mean_absolute_error: 0.3797 - cosine_proximity: -0.375 - ETA: 1:13 - loss: 2.2519 - acc: 0.6094 - mean_absolute_error: 0.3941 - cosine_proximity: -0.390 - ETA: 1:04 - loss: 2.1395 - acc: 0.6116 - mean_absolute_error: 0.3934 - cosine_proximity: -0.388 - ETA: 57s - loss: 2.0376 - acc: 0.6152 - mean_absolute_error: 0.3917 - cosine_proximity: -0.384 - ETA: 51s - loss: 1.9419 - acc: 0.6198 - mean_absolute_error: 0.3903 - cosine_proximity: -0.38 - ETA: 47s - loss: 1.8672 - acc: 0.6156 - mean_absolute_error: 0.3920 - cosine_proximity: -0.38 - ETA: 43s - loss: 1.7769 - acc: 0.6236 - mean_absolute_error: 0.3889 - cosine_proximity: -0.37 - ETA: 40s - loss: 1.7243 - acc: 0.6198 - mean_absolute_error: 0.3920 - cosine_proximity: -0.38 - ETA: 38s - loss: 1.6734 - acc: 0.6190 - mean_absolute_error: 0.3933 - cosine_proximity: -0.38 - ETA: 36s - loss: 1.6316 - acc: 0.6172 - mean_absolute_error: 0.3957 - cosine_proximity: -0.38 - ETA: 34s - loss: 1.5840 - acc: 0.6188 - mean_absolute_error: 0.3949 - cosine_proximity: -0.38 - ETA: 32s - loss: 1.5443 - acc: 0.6221 - mean_absolute_error: 0.3940 - cosine_proximity: -0.37 - ETA: 31s - loss: 1.5236 - acc: 0.6149 - mean_absolute_error: 0.3992 - cosine_proximity: -0.38 - ETA: 29s - loss: 1.4921 - acc: 0.6137 - mean_absolute_error: 0.3998 - cosine_proximity: -0.38 - ETA: 28s - loss: 1.4506 - acc: 0.6184 - mean_absolute_error: 0.3977 - cosine_proximity: -0.38 - ETA: 27s - loss: 1.4238 - acc: 0.6188 - mean_absolute_error: 0.3985 - cosine_proximity: -0.38 - ETA: 26s - loss: 1.4074 - acc: 0.6153 - mean_absolute_error: 0.4009 - cosine_proximity: -0.38 - ETA: 25s - loss: 1.3879 - acc: 0.6143 - mean_absolute_error: 0.4019 - cosine_proximity: -0.38 - ETA: 24s - loss: 1.3647 - acc: 0.6141 - mean_absolute_error: 0.4029 - cosine_proximity: -0.38 - ETA: 24s - loss: 1.3429 - acc: 0.6172 - mean_absolute_error: 0.4031 - cosine_proximity: -0.38 - ETA: 23s - loss: 1.3209 - acc: 0.6206 - mean_absolute_error: 0.4031 - cosine_proximity: -0.38 - ETA: 22s - loss: 1.3062 - acc: 0.6208 - mean_absolute_error: 0.4046 - cosine_proximity: -0.38 - ETA: 21s - loss: 1.2896 - acc: 0.6215 - mean_absolute_error: 0.4056 - cosine_proximity: -0.38 - ETA: 21s - loss: 1.2736 - acc: 0.6267 - mean_absolute_error: 0.4061 - cosine_proximity: -0.38 - ETA: 20s - loss: 1.2591 - acc: 0.6293 - mean_absolute_error: 0.4069 - cosine_proximity: -0.38 - ETA: 20s - loss: 1.2436 - acc: 0.6292 - mean_absolute_error: 0.4074 - cosine_proximity: -0.38 - ETA: 19s - loss: 1.2262 - acc: 0.6316 - mean_absolute_error: 0.4074 - cosine_proximity: -0.38 - ETA: 18s - loss: 1.2144 - acc: 0.6318 - mean_absolute_error: 0.4080 - cosine_proximity: -0.37 - ETA: 18s - loss: 1.2018 - acc: 0.6321 - mean_absolute_error: 0.4090 - cosine_proximity: -0.37 - ETA: 17s - loss: 1.1896 - acc: 0.6328 - mean_absolute_error: 0.4093 - cosine_proximity: -0.37 - ETA: 17s - loss: 1.1788 - acc: 0.6326 - mean_absolute_error: 0.4098 - cosine_proximity: -0.37 - ETA: 17s - loss: 1.1675 - acc: 0.6341 - mean_absolute_error: 0.4097 - cosine_proximity: -0.38 - ETA: 16s - loss: 1.1632 - acc: 0.6339 - mean_absolute_error: 0.4115 - cosine_proximity: -0.38 - ETA: 16s - loss: 1.1516 - acc: 0.6357 - mean_absolute_error: 0.4117 - cosine_proximity: -0.38 - ETA: 15s - loss: 1.1396 - acc: 0.6362 - mean_absolute_error: 0.4115 - cosine_proximity: -0.37 - ETA: 15s - loss: 1.1286 - acc: 0.6383 - mean_absolute_error: 0.4115 - cosine_proximity: -0.37 - ETA: 15s - loss: 1.1216 - acc: 0.6380 - mean_absolute_error: 0.4122 - cosine_proximity: -0.37 - ETA: 14s - loss: 1.1147 - acc: 0.6343 - mean_absolute_error: 0.4134 - cosine_proximity: -0.37 - ETA: 14s - loss: 1.1096 - acc: 0.6341 - mean_absolute_error: 0.4141 - cosine_proximity: -0.37 - ETA: 14s - loss: 1.1047 - acc: 0.6317 - mean_absolute_error: 0.4152 - cosine_proximity: -0.37 - ETA: 13s - loss: 1.1010 - acc: 0.6292 - mean_absolute_error: 0.4165 - cosine_proximity: -0.37 - ETA: 13s - loss: 1.0945 - acc: 0.6294 - mean_absolute_error: 0.4171 - cosine_proximity: -0.37 - ETA: 13s - loss: 1.0890 - acc: 0.6293 - mean_absolute_error: 0.4178 - cosine_proximity: -0.37 - ETA: 12s - loss: 1.0824 - acc: 0.6305 - mean_absolute_error: 0.4179 - cosine_proximity: -0.37 - ETA: 12s - loss: 1.0770 - acc: 0.6314 - mean_absolute_error: 0.4185 - cosine_proximity: -0.37 - ETA: 12s - loss: 1.0715 - acc: 0.6319 - mean_absolute_error: 0.4189 - cosine_proximity: -0.37 - ETA: 12s - loss: 1.0660 - acc: 0.6311 - mean_absolute_error: 0.4192 - cosine_proximity: -0.37 - ETA: 11s - loss: 1.0639 - acc: 0.6289 - mean_absolute_error: 0.4205 - cosine_proximity: -0.37 - ETA: 11s - loss: 1.0584 - acc: 0.6297 - mean_absolute_error: 0.4204 - cosine_proximity: -0.37 - ETA: 11s - loss: 1.0537 - acc: 0.6302 - mean_absolute_error: 0.4208 - cosine_proximity: -0.37 - ETA: 11s - loss: 1.0497 - acc: 0.6298 - mean_absolute_error: 0.4210 - cosine_proximity: -0.37 - ETA: 10s - loss: 1.0430 - acc: 0.6303 - mean_absolute_error: 0.4208 - cosine_proximity: -0.37 - ETA: 10s - loss: 1.0369 - acc: 0.6324 - mean_absolute_error: 0.4204 - cosine_proximity: -0.37 - ETA: 10s - loss: 1.0341 - acc: 0.6312 - mean_absolute_error: 0.4210 - cosine_proximity: -0.37 - ETA: 10s - loss: 1.0311 - acc: 0.6322 - mean_absolute_error: 0.4213 - cosine_proximity: -0.37 - ETA: 10s - loss: 1.0260 - acc: 0.6336 - mean_absolute_error: 0.4213 - cosine_proximity: -0.37 - ETA: 9s - loss: 1.0229 - acc: 0.6342 - mean_absolute_error: 0.4220 - cosine_proximity: -0.3724 - ETA: 9s - loss: 1.0196 - acc: 0.6338 - mean_absolute_error: 0.4222 - cosine_proximity: -0.373 - ETA: 9s - loss: 1.0150 - acc: 0.6352 - mean_absolute_error: 0.4222 - cosine_proximity: -0.372 - ETA: 9s - loss: 1.0114 - acc: 0.6365 - mean_absolute_error: 0.4220 - cosine_proximity: -0.373 - ETA: 9s - loss: 1.0061 - acc: 0.6380 - mean_absolute_error: 0.4215 - cosine_proximity: -0.371 - ETA: 8s - loss: 1.0031 - acc: 0.6373 - mean_absolute_error: 0.4218 - cosine_proximity: -0.371 - ETA: 8s - loss: 0.9991 - acc: 0.6383 - mean_absolute_error: 0.4213 - cosine_proximity: -0.373 - ETA: 8s - loss: 0.9954 - acc: 0.6386 - mean_absolute_error: 0.4215 - cosine_proximity: -0.373 - ETA: 8s - loss: 0.9926 - acc: 0.6368 - mean_absolute_error: 0.4219 - cosine_proximity: -0.372 - ETA: 7s - loss: 0.9886 - acc: 0.6384 - mean_absolute_error: 0.4217 - cosine_proximity: -0.371 - ETA: 7s - loss: 0.9855 - acc: 0.6375 - mean_absolute_error: 0.4219 - cosine_proximity: -0.371 - ETA: 7s - loss: 0.9826 - acc: 0.6367 - mean_absolute_error: 0.4222 - cosine_proximity: -0.370 - ETA: 7s - loss: 0.9798 - acc: 0.6381 - mean_absolute_error: 0.4222 - cosine_proximity: -0.371 - ETA: 7s - loss: 0.9766 - acc: 0.6377 - mean_absolute_error: 0.4224 - cosine_proximity: -0.370 - ETA: 6s - loss: 0.9734 - acc: 0.6385 - mean_absolute_error: 0.4223 - cosine_proximity: -0.369 - ETA: 6s - loss: 0.9709 - acc: 0.6396 - mean_absolute_error: 0.4224 - cosine_proximity: -0.370 - ETA: 6s - loss: 0.9673 - acc: 0.6404 - mean_absolute_error: 0.4222 - cosine_proximity: -0.369 - ETA: 6s - loss: 0.9645 - acc: 0.6414 - mean_absolute_error: 0.4221 - cosine_proximity: -0.370 - ETA: 5s - loss: 0.9630 - acc: 0.6416 - mean_absolute_error: 0.4222 - cosine_proximity: -0.372 - ETA: 5s - loss: 0.9614 - acc: 0.6426 - mean_absolute_error: 0.4221 - cosine_proximity: -0.373 - ETA: 5s - loss: 0.9582 - acc: 0.6431 - mean_absolute_error: 0.4222 - cosine_proximity: -0.371 - ETA: 5s - loss: 0.9556 - acc: 0.6437 - mean_absolute_error: 0.4222 - cosine_proximity: -0.371 - ETA: 5s - loss: 0.9537 - acc: 0.6438 - mean_absolute_error: 0.4222 - cosine_proximity: -0.372 - ETA: 4s - loss: 0.9519 - acc: 0.6430 - mean_absolute_error: 0.4226 - cosine_proximity: -0.372 - ETA: 4s - loss: 0.9488 - acc: 0.6450 - mean_absolute_error: 0.4222 - cosine_proximity: -0.3719\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7200/7200 [==============================] - ETA: 4s - loss: 0.9472 - acc: 0.6444 - mean_absolute_error: 0.4223 - cosine_proximity: -0.371 - ETA: 4s - loss: 0.9450 - acc: 0.6451 - mean_absolute_error: 0.4223 - cosine_proximity: -0.371 - ETA: 4s - loss: 0.9429 - acc: 0.6449 - mean_absolute_error: 0.4225 - cosine_proximity: -0.370 - ETA: 3s - loss: 0.9401 - acc: 0.6455 - mean_absolute_error: 0.4223 - cosine_proximity: -0.370 - ETA: 3s - loss: 0.9396 - acc: 0.6455 - mean_absolute_error: 0.4228 - cosine_proximity: -0.370 - ETA: 3s - loss: 0.9377 - acc: 0.6456 - mean_absolute_error: 0.4228 - cosine_proximity: -0.370 - ETA: 3s - loss: 0.9362 - acc: 0.6457 - mean_absolute_error: 0.4227 - cosine_proximity: -0.369 - ETA: 3s - loss: 0.9342 - acc: 0.6465 - mean_absolute_error: 0.4227 - cosine_proximity: -0.369 - ETA: 3s - loss: 0.9327 - acc: 0.6466 - mean_absolute_error: 0.4227 - cosine_proximity: -0.369 - ETA: 2s - loss: 0.9309 - acc: 0.6462 - mean_absolute_error: 0.4226 - cosine_proximity: -0.369 - ETA: 2s - loss: 0.9291 - acc: 0.6471 - mean_absolute_error: 0.4225 - cosine_proximity: -0.369 - ETA: 2s - loss: 0.9273 - acc: 0.6474 - mean_absolute_error: 0.4225 - cosine_proximity: -0.368 - ETA: 2s - loss: 0.9256 - acc: 0.6472 - mean_absolute_error: 0.4226 - cosine_proximity: -0.368 - ETA: 2s - loss: 0.9234 - acc: 0.6485 - mean_absolute_error: 0.4223 - cosine_proximity: -0.369 - ETA: 2s - loss: 0.9234 - acc: 0.6475 - mean_absolute_error: 0.4228 - cosine_proximity: -0.369 - ETA: 1s - loss: 0.9220 - acc: 0.6473 - mean_absolute_error: 0.4229 - cosine_proximity: -0.369 - ETA: 1s - loss: 0.9212 - acc: 0.6468 - mean_absolute_error: 0.4232 - cosine_proximity: -0.369 - ETA: 1s - loss: 0.9198 - acc: 0.6467 - mean_absolute_error: 0.4230 - cosine_proximity: -0.370 - ETA: 1s - loss: 0.9183 - acc: 0.6462 - mean_absolute_error: 0.4232 - cosine_proximity: -0.370 - ETA: 1s - loss: 0.9162 - acc: 0.6467 - mean_absolute_error: 0.4230 - cosine_proximity: -0.369 - ETA: 1s - loss: 0.9156 - acc: 0.6467 - mean_absolute_error: 0.4231 - cosine_proximity: -0.370 - ETA: 0s - loss: 0.9129 - acc: 0.6471 - mean_absolute_error: 0.4227 - cosine_proximity: -0.369 - ETA: 0s - loss: 0.9119 - acc: 0.6466 - mean_absolute_error: 0.4227 - cosine_proximity: -0.369 - ETA: 0s - loss: 0.9101 - acc: 0.6468 - mean_absolute_error: 0.4225 - cosine_proximity: -0.369 - ETA: 0s - loss: 0.9086 - acc: 0.6476 - mean_absolute_error: 0.4224 - cosine_proximity: -0.370 - ETA: 0s - loss: 0.9078 - acc: 0.6474 - mean_absolute_error: 0.4226 - cosine_proximity: -0.369 - ETA: 0s - loss: 0.9065 - acc: 0.6479 - mean_absolute_error: 0.4225 - cosine_proximity: -0.370 - 19s 3ms/step - loss: 0.9058 - acc: 0.6481 - mean_absolute_error: 0.4224 - cosine_proximity: -0.3704 - val_loss: 0.7746 - val_acc: 0.6375 - val_mean_absolute_error: 0.4282 - val_cosine_proximity: -0.3812\n"
     ]
    }
   ],
   "source": [
    "model_trained = model.fit(X_train_dataset, Y_train.values,\n",
    "                          validation_split=0.1, batch_size=model_parameters['batch_size'], epochs=1,\n",
    "                          class_weight={1: ones_weight, 0: 1})\n",
    "\n",
    "model.save(\"results/\" + now + \"_model.h5\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Wczytywanie modelu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import load_model\n",
    "from keras import backend as K\n",
    "\n",
    "#read tokenizer\n",
    "model = load_model('results/20190410-140449model.h5', custom_objects={'exponent_neg_manhattan_distance': exponent_neg_manhattan_distance})\n",
    "with open('results/20190410-140449_tokenizer_warsztaty.pickle', 'rb') as handle:\n",
    "    tokenizer = pickle.load(handle)\n",
    "    \n",
    "X_validation = X_validation.drop([\"question1_tokens\",\"question2_tokens\"], axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Task 1: Tokenize validation data and prepare it for making a prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th>question1</th>\n",
       "      <th>question2</th>\n",
       "      <th>question1_cleared</th>\n",
       "      <th>question2_cleared</th>\n",
       "      <th>question1_tokens_new</th>\n",
       "      <th>question2_tokens_new</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>qid1</th>\n",
       "      <th>qid2</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>13684</th>\n",
       "      <th>13685</th>\n",
       "      <td>What is the essential reading list for learnin...</td>\n",
       "      <td>What books do you recommend to read about Sema...</td>\n",
       "      <td>[what, is, the, essential, reading, list, for,...</td>\n",
       "      <td>[what, book, do, you, recommend, to, read, abo...</td>\n",
       "      <td>[3, 4, 2, 1639, 844, 539, 15, 189, 48, 2, 7226...</td>\n",
       "      <td>[3, 91, 10, 16, 1185, 8, 244, 48, 7226, 268]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13977</th>\n",
       "      <th>13978</th>\n",
       "      <td>Can you treat pneumonia with Albuterol?</td>\n",
       "      <td>How can you treat pneumonia and albuterol?</td>\n",
       "      <td>[can, you, treat, pneumonia, with, albuterol]</td>\n",
       "      <td>[how, can, you, treat, pneumonia, and, albuterol]</td>\n",
       "      <td>[14, 16, 922, 4753, 29, 7260]</td>\n",
       "      <td>[7, 14, 16, 922, 4753, 13, 7260]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16126</th>\n",
       "      <th>8887</th>\n",
       "      <td>If you voted for Donald Trump, why did you vot...</td>\n",
       "      <td>Why did you specifically vote for Donald Trump?</td>\n",
       "      <td>[if, you, voted, for, donald, trump, why, did,...</td>\n",
       "      <td>[why, did, you, specifically, vote, for, donal...</td>\n",
       "      <td>[25, 16, 2485, 15, 153, 100, 17, 54, 16, 566, ...</td>\n",
       "      <td>[17, 54, 16, 2826, 566, 15, 153, 100]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16179</th>\n",
       "      <th>16180</th>\n",
       "      <td>Can you see who viewed your Instagram?</td>\n",
       "      <td>Can someone see if you have viewed public Inst...</td>\n",
       "      <td>[can, you, see, who, viewed, your, instagram]</td>\n",
       "      <td>[can, someone, see, if, you, have, viewed, pub...</td>\n",
       "      <td>[14, 16, 190, 44, 1379, 35, 220]</td>\n",
       "      <td>[14, 94, 190, 25, 16, 27, 1379, 509, 220]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2209</th>\n",
       "      <th>5609</th>\n",
       "      <td>Quora: How do you post a question on Quora?</td>\n",
       "      <td>How do I post something in Quora?</td>\n",
       "      <td>[quora, how, do, you, post, a, question, on, q...</td>\n",
       "      <td>[how, do, i, post, something, in, quora]</td>\n",
       "      <td>[62, 7, 10, 16, 374, 5, 79, 21, 62]</td>\n",
       "      <td>[7, 10, 6, 374, 588, 9, 62]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                     question1  \\\n",
       "qid1  qid2                                                       \n",
       "13684 13685  What is the essential reading list for learnin...   \n",
       "13977 13978            Can you treat pneumonia with Albuterol?   \n",
       "16126 8887   If you voted for Donald Trump, why did you vot...   \n",
       "16179 16180             Can you see who viewed your Instagram?   \n",
       "2209  5609         Quora: How do you post a question on Quora?   \n",
       "\n",
       "                                                     question2  \\\n",
       "qid1  qid2                                                       \n",
       "13684 13685  What books do you recommend to read about Sema...   \n",
       "13977 13978         How can you treat pneumonia and albuterol?   \n",
       "16126 8887     Why did you specifically vote for Donald Trump?   \n",
       "16179 16180  Can someone see if you have viewed public Inst...   \n",
       "2209  5609                   How do I post something in Quora?   \n",
       "\n",
       "                                             question1_cleared  \\\n",
       "qid1  qid2                                                       \n",
       "13684 13685  [what, is, the, essential, reading, list, for,...   \n",
       "13977 13978      [can, you, treat, pneumonia, with, albuterol]   \n",
       "16126 8887   [if, you, voted, for, donald, trump, why, did,...   \n",
       "16179 16180      [can, you, see, who, viewed, your, instagram]   \n",
       "2209  5609   [quora, how, do, you, post, a, question, on, q...   \n",
       "\n",
       "                                             question2_cleared  \\\n",
       "qid1  qid2                                                       \n",
       "13684 13685  [what, book, do, you, recommend, to, read, abo...   \n",
       "13977 13978  [how, can, you, treat, pneumonia, and, albuterol]   \n",
       "16126 8887   [why, did, you, specifically, vote, for, donal...   \n",
       "16179 16180  [can, someone, see, if, you, have, viewed, pub...   \n",
       "2209  5609            [how, do, i, post, something, in, quora]   \n",
       "\n",
       "                                          question1_tokens_new  \\\n",
       "qid1  qid2                                                       \n",
       "13684 13685  [3, 4, 2, 1639, 844, 539, 15, 189, 48, 2, 7226...   \n",
       "13977 13978                      [14, 16, 922, 4753, 29, 7260]   \n",
       "16126 8887   [25, 16, 2485, 15, 153, 100, 17, 54, 16, 566, ...   \n",
       "16179 16180                   [14, 16, 190, 44, 1379, 35, 220]   \n",
       "2209  5609                 [62, 7, 10, 16, 374, 5, 79, 21, 62]   \n",
       "\n",
       "                                     question2_tokens_new  \n",
       "qid1  qid2                                                 \n",
       "13684 13685  [3, 91, 10, 16, 1185, 8, 244, 48, 7226, 268]  \n",
       "13977 13978              [7, 14, 16, 922, 4753, 13, 7260]  \n",
       "16126 8887          [17, 54, 16, 2826, 566, 15, 153, 100]  \n",
       "16179 16180     [14, 94, 190, 25, 16, 27, 1379, 509, 220]  \n",
       "2209  5609                    [7, 10, 6, 374, 588, 9, 62]  "
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_validation.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2000/2000 [==============================] - ETA: 34 - ETA: 8 - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - 2s 896us/step\n",
      "[0.5896039743423462, 0.686, 0.4014213891029358, -0.369499968290329]\n"
     ]
    }
   ],
   "source": [
    "print(model.evaluate(X_val_dataset, Y_validation.values))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Liczba poprawnie przewidzianych ogłoszeń:  1372\n",
      "Liczba wszystkich ogłoszeń w zbiorze testowym:  2000\n",
      "Confusion matrix: \n",
      "Predicted  False  True\n",
      "Actual                \n",
      "False        724   537\n",
      "True          91   648\n",
      "Metryki: \n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.89      0.57      0.70      1261\n",
      "           1       0.55      0.88      0.67       739\n",
      "\n",
      "   micro avg       0.69      0.69      0.69      2000\n",
      "   macro avg       0.72      0.73      0.69      2000\n",
      "weighted avg       0.76      0.69      0.69      2000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "preds = model.predict(X_val_dataset)\n",
    "preds_binary = calculate_preds_binary(preds)\n",
    "\n",
    "cm, metrics = model_statistics(preds_binary, Y_validation)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Task 2: Display first 5 correctly and incorrectly classified pairs of questions"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py36",
   "language": "python",
   "name": "py36"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
