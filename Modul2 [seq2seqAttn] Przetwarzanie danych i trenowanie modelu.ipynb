{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/agatawlaszczyk/anaconda3/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd \n",
    "import numpy as np \n",
    "from utils.text_helpers import *\n",
    "from layers.attention import AttentionDecoder"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Wczytanie i przetworzenie danych\n",
    "Początkowo wczytujemy dane z przygotowanego pliku. Rozdzielamy je na zbiór treningowy, zbiór walidacyjny i zbiór testowy. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#do we want to analyze the job_content or job_name? \n",
    "dataset_details = {\n",
    "    'data_focus': 'job_content',\n",
    "    'max_number_words': 20000,\n",
    "    'max_seq_len': 250\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = Data(dataset_details['data_focus'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "17000 3000 1500\n",
      "True\n"
     ]
    }
   ],
   "source": [
    "print(len(data.train_X), len(data.val_X), len(data.test_X))\n",
    "#sprawdzamy czy wszystkie zbiory zawierają te same kategorie \n",
    "print(set(pd.unique(data.train_y))==set(pd.unique(data.val_y)) and set(pd.unique(data.val_y))==set(pd.unique(data.test_y)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Opis:  firma wenglorz jako jedyna na polskim rynku wykonuje kompleksowo obiekty wytwórni pasz od projektu technologicznego i budowlanego przez produkcję konstrukcji i urządzeń po montaż i uruchomienie. zajmuje się także projektowaniem produkukowaniem oraz montażem kompletnych linii technologicznych wraz z ich rozruchem. . opis stanowiska pracy umiejętność czytania dokumentacji technicznej wykonawczej oraz warsztatowej opracowywanie na podstawie dokumentacji zestawień elementów przeznaczonych do zamówienia opracowanie plików w formacie nc dstv dla maszyn sterowanych numerycznie ulepszanie istniejących rozwiązań technicznych wprowadzenie nowych technologii innowacji usprawnień bieżący nadzór nad wykonywaniem zleceń produkcyjnych kontrola zgodności dokumentacji produkcyjnej z odpowiednimi standardami przepisami oraz normami technicznymi i normami bezpieczeństwa rozwój istniejących rozwiązań technicznych oraz wprowadzanie nowych technologii wymagania wykształcenie wyższe techniczne znajomość i umiejętność wykorzystania w pracy programów cad autocad draft sight certyfikaty będą dodatkowym atutem odpowiedzialność za terminowe i właściwe wykonywanie przydzielonych obowiązków sumienność w wykonywaniu powierzonych zadań dyspozycyjność umiejętność pracy w zespole mile widziane znajomość języka angielskiego min. na poziomie podstawowym oferujemy pracę w firmie o ugruntowanej pozycji na rynku umowę o pracę w pełnym wymiarze czasowym możliwość podnoszenia kwalifikacji zawodowych wynagrodzenie adekwatne do wiedzy i umiejętności\n",
      "Kategoria:  Produkcja\n"
     ]
    }
   ],
   "source": [
    "example_id=24\n",
    "print(\"Opis: \", data.train_X.values[example_id])\n",
    "print(\"Kategoria: \", data.train_y.values[example_id])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Przygotowanie tekstu"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Zanim stworzymy model i wytrenujemy go, musimy odpowiednio przetworzyć tekst, żeby dostosować reprezentację tekstową do reprezentacji akceptowalnej przez model (tensory). <br>\n",
    "\n",
    "Kroki preprocessingu: \n",
    "1. lemmatyzacja tekstu\n",
    "2. dodanie tokenu kończącego tekst do każdego wpisu\\\n",
    "3. tokenizacja tekstu\n",
    "4. text to vector\n",
    "5. dodanie znaków początkowych i końcowych dla kategorii\n",
    "6. transformacja kategorii do ich domyślnej reprezentacji dla modelu \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Oryginalny tekst: firma wenglorz jako jedyna na polskim rynku wykonuje kompleksowo obiekty wytwórni pasz od projektu technologicznego i budowlanego przez produkcję konstrukcji i urządzeń po montaż i uruchomienie. zajmuje się także projektowaniem produkukowaniem oraz montażem kompletnych linii technologicznych wraz z ich rozruchem. . opis stanowiska pracy umiejętność czytania dokumentacji technicznej wykonawczej oraz warsztatowej opracowywanie na podstawie dokumentacji zestawień elementów przeznaczonych do zamówienia opracowanie plików w formacie nc dstv dla maszyn sterowanych numerycznie ulepszanie istniejących rozwiązań technicznych wprowadzenie nowych technologii innowacji usprawnień bieżący nadzór nad wykonywaniem zleceń produkcyjnych kontrola zgodności dokumentacji produkcyjnej z odpowiednimi standardami przepisami oraz normami technicznymi i normami bezpieczeństwa rozwój istniejących rozwiązań technicznych oraz wprowadzanie nowych technologii wymagania wykształcenie wyższe techniczne znajomość i umiejętność wykorzystania w pracy programów cad autocad draft sight certyfikaty będą dodatkowym atutem odpowiedzialność za terminowe i właściwe wykonywanie przydzielonych obowiązków sumienność w wykonywaniu powierzonych zadań dyspozycyjność umiejętność pracy w zespole mile widziane znajomość języka angielskiego min. na poziomie podstawowym oferujemy pracę w firmie o ugruntowanej pozycji na rynku umowę o pracę w pełnym wymiarze czasowym możliwość podnoszenia kwalifikacji zawodowych wynagrodzenie adekwatne do wiedzy i umiejętności\n",
      "**********\n",
      "Wynik kroków 1 i 2: firma wenglorz jaka jedyna polska rynek wykonywać kompleksowo obiekt wytwórnia pasać projekt technologiczny budowlany przez produkcja konstrukcja urządzenie montaż uruchomić . zajmować się także projektować produkukowaniem oraz montaż kompletny linia technologiczny wraz on rozruch . opis stanowisko praca umiejętność czytać dokumentacja techniczny wykonawczy oraz warsztatowy opracowywać podstawa dokumentacja zestawić element przeznaczyć zamówić opracować plik format dstv dla maszyna sterować numerycznie ulepszać istnieć rozwiązać techniczny wprowadzenie nowa technologia innowacja usprawnić bieżący nadzór nad wykonywać zlecenie produkcyjny kontrola zgodność dokumentacja produkcyjny odpowiedni standard przepis oraz norma techniczny norma bezpieczeństwo rozwój istnieć rozwiązać techniczny oraz wprowadzać nowa technologia wymagać wykształcenie wysoki techniczny znajomość umiejętność wykorzystać praca program cad autocad draft sight certyfikat być dodatkowy atut odpowiedzialność terminowy właściwy wykonywać przydzielić obowiązek sumienność wykonywać powierzyć zadać dyspozycyjność umiejętność praca zespół mila widzieć znajomość język angielski min . poziom podstawowy oferować praca firma ugruntować pozycja rynek umowa praca pełny wymiar czasowy możliwość podnosić kwalifikacja zawodowy wynagrodzenie adekwatny wiedza umiejętność <eos>\n",
      "**********\n",
      "Wynik kroków 3 i 4: [    4  7849   104  2590    31    26   100  3149   505  2863  3283    34\n",
      "   418   202   148   112   716   121   555  1218   257    14   287   166\n",
      " 18540     3   555  2467   581   418   520   125  3764   291     6     1\n",
      "     8   492    73    43  1238     3  2864   290   182    73   630   553\n",
      "  1725   265   727  2396  2232 18541    24   178   918  3417  3114   358\n",
      "    78    43  1826    53   115  1749  1106   134   113    98   100   342\n",
      "    84   126   731    73    84   365   164   235     3   769    43   769\n",
      "   345    12   358    78    43     3   380    53   115    49    25    13\n",
      "    43     2     8   372     1    86  1100   841 18542 18543   469     9\n",
      "    74    93   107   590   626   100  1639    67   323   100   229    54\n",
      "   144     8     1    15    46    45     2    18    32    82   133   209\n",
      "    38     1     4   193   123    26    41     1   105   534  1952    22\n",
      "   214   154    28    47   339    90     8    10     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0]\n"
     ]
    }
   ],
   "source": [
    "#1 i 2: lemmatyzacja + dodanie tokenu konczacego zdanie\n",
    "datasets = [data.train_X, data.val_X, data.test_X]\n",
    "print(\"Oryginalny tekst: {}\".format(data.train_X.values[example_id]))\n",
    "\n",
    "for i,dataset in enumerate(datasets):\n",
    "    datasets[i] = dataset.apply(lambda x: data.lemmatize(x) + \" <eos>\")\n",
    "print(\"*\"*10)\n",
    "print(\"Wynik kroków 1 i 2: {}\".format(datasets[0].values[example_id]))    \n",
    "\n",
    "#3: tokenizacja tekstu \n",
    "data.fit_tokenizer(datasets[0].values, max_number_words=dataset_details['max_number_words'])\n",
    "\n",
    "#4: text to vector\n",
    "for i, dataset in enumerate(datasets):\n",
    "    datasets[i] = data.text_to_vector(dataset, max_seq_len=dataset_details['max_seq_len'])\n",
    "print(\"*\"*10)\n",
    "print(\"Wynik kroków 3 i 4: {}\".format(datasets[0][example_id]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initial labels: Produkcja\n",
      "After step 5: ['\\t', 'Produkcja', '\\n']\n",
      "**********\n",
      "Input after step 6: [[1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "  0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "  0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "  0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]]\n",
      "Output after step 6: [[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "  0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "  0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "  0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]]\n"
     ]
    }
   ],
   "source": [
    "#5: dodanie znaków początkowych i końcowych do kategorii \n",
    "labels = [data.train_y, data.val_y, data.test_y]\n",
    "print(\"Initial labels: {}\".format(labels[0].values[example_id]))\n",
    "for i,label_set in enumerate(labels): \n",
    "    labels[i] = label_set.apply(lambda x: (\"\\t \" + x.replace(\" \", \"_\") + \" \\n\").split(\" \"))\n",
    "print(\"After step 5: {}\".format(labels[0].values[example_id]))\n",
    "print(\"*\"*10)\n",
    "\n",
    "#6 transformacja do reprezentacji domyślnej dla modelu \n",
    "labels_input = []\n",
    "labels_output = []\n",
    "for i, label_set in enumerate(labels): \n",
    "    transformed_input, transformed_output = data.transform_labels(label_set)\n",
    "    labels_input.append(transformed_input)\n",
    "    labels_output.append(transformed_output)\n",
    "print(\"Input after step 6: {}\".format(labels_input[0][example_id]))\n",
    "print(\"Output after step 6: {}\".format(labels_output[0][example_id]))\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Mając przystosowane reprezentacje tekstowe, możemy stworzyć schemat modelu. Model zaimplementowany jest w Kerasie. Składa się z trzech głównych komponentów: \n",
    "- encodera\n",
    "- decodera\n",
    "- atencji. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import load_model, Model\n",
    "from keras.layers import Dense, Dropout, Embedding, LSTM, Bidirectional, Input, Concatenate, TimeDistributed, GRU\n",
    "from keras.initializers import Constant"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#additional network info\n",
    "network = Network({\n",
    "    'embedding_dim': 300,\n",
    "    'embeddings': load_word_embeddings('./data/wiki.pl.vec'),\n",
    "    'train_embeddings': True,\n",
    "    'latent_dim': dataset_details['max_seq_len'],\n",
    "    'dropout': 0.5, \n",
    "    'num_decoder_tokens': 37,\n",
    "    'batch_size': 500, \n",
    "    'num_epochs': 5,\n",
    "    'return_probabilities': False,\n",
    "    'max_labels': 37\n",
    "})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(20001, 300)\n"
     ]
    }
   ],
   "source": [
    "embed_matrix = generate_embedding_matrix(network.embeddings, data.tokenizer.word_index)\n",
    "print(embed_matrix.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder_inputs = Input(shape=(None,), name='encoder_input')\n",
    "embedding_layer = Embedding(input_dim=dataset_details['max_number_words']+1,\n",
    "                            output_dim=network.embedding_dim,\n",
    "                            embeddings_initializer = Constant(embed_matrix),\n",
    "                            trainable=network.train_embeddings, name='embeddings')  \n",
    "embedded_sequences = embedding_layer(encoder_inputs)\n",
    "encoder = Bidirectional(LSTM(network.latent_dim, return_sequences=True, dropout=network.dropout), name='encoder')\n",
    "states =  encoder(embedded_sequences)\n",
    "outputs_true = Input(shape=(None, None,), dtype='int64', name='decoder_input')\n",
    "decoder_outputs = AttentionDecoder(network.latent_dim*2, network.max_labels, return_probabilities=network.return_probabilities, name='attention')([states, outputs_true], use_teacher_forcing=False)\n",
    "model = Model([encoder_inputs, outputs_true], decoder_outputs)\n",
    "\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer='adam', loss='categorical_crossentropy',\n",
    "              metrics=['categorical_accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "encoder_input (InputLayer)      (None, None)         0                                            \n",
      "__________________________________________________________________________________________________\n",
      "embeddings (Embedding)          (None, None, 300)    6000300     encoder_input[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "encoder (Bidirectional)         (None, None, 500)    1102000     embeddings[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "decoder_input (InputLayer)      (None, None, None)   0                                            \n",
      "__________________________________________________________________________________________________\n",
      "attention (AttentionDecoder)    (None, None, 37)     2336787     encoder[0][0]                    \n",
      "                                                                 decoder_input[0][0]              \n",
      "==================================================================================================\n",
      "Total params: 9,439,087\n",
      "Trainable params: 9,439,087\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 17000 samples, validate on 3000 samples\n",
      "Epoch 1/5\n",
      "17000/17000 [==============================] - 516s 30ms/step - loss: 0.1041 - categorical_accuracy: 0.9682 - val_loss: 0.2515 - val_categorical_accuracy: 0.9352\n",
      "Epoch 2/5\n",
      "17000/17000 [==============================] - 492s 29ms/step - loss: 0.0959 - categorical_accuracy: 0.9704 - val_loss: 0.2638 - val_categorical_accuracy: 0.9310\n",
      "Epoch 3/5\n",
      "17000/17000 [==============================] - 492s 29ms/step - loss: 0.0832 - categorical_accuracy: 0.9740 - val_loss: 0.2742 - val_categorical_accuracy: 0.9322\n"
     ]
    }
   ],
   "source": [
    "import keras.callbacks as callback \n",
    "history = model.fit([datasets[0], labels_input[0]], labels_output[0],\n",
    "              batch_size=network.batch_size,\n",
    "              epochs=network.num_epochs,\n",
    "              validation_data=[[datasets[1], labels_input[1]], labels_output[1]], verbose=1, \n",
    "                    callbacks=[callback.EarlyStopping(patience=2, restore_best_weights = True)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "save = True\n",
    "path_to_model = './data/model_seq2seq_{}.h5'.format(dataset_details['data_focus'])\n",
    "if save:\n",
    "    model.save(path_to_model)\n",
    "    model.save_weights(path_to_model.replace('.h5', '_weights.h5'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 0.8898 (bez atencji po 5 epokach na val)\n",
    "# 0.9244 (z atencją po 5 epokach na val)\n",
    "# 0.9332 (z atencją po 10 epokach na val)\n",
    "# 0.9368 (z atencją po 15 epokach)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "encoder_input (InputLayer)      (None, None)         0                                            \n",
      "__________________________________________________________________________________________________\n",
      "embeddings (Embedding)          (None, None, 300)    6000300     encoder_input[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "encoder (Bidirectional)         (None, None, 500)    1102000     embeddings[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "decoder_input (InputLayer)      (None, None, None)   0                                            \n",
      "__________________________________________________________________________________________________\n",
      "attention (AttentionDecoder)    (None, None, 37)     2336787     encoder[0][0]                    \n",
      "                                                                 decoder_input[0][0]              \n",
      "==================================================================================================\n",
      "Total params: 9,439,087\n",
      "Trainable params: 9,439,087\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
