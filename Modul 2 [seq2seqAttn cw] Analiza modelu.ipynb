{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Seq2Seq z atencją - analiza modelu"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cześć! \n",
    "Ten notebook stworzony jest do eksploracji i praktycznego wykorzystania możliwości modelu *seq2seq z mechanizmem atencji*!. Dzięki przerobieniu wszystkich proponowanych przez nas zadań, dowiesz się jak: \n",
    "- wczytać przetrenowany model \n",
    "- wygenerować predykcje z modelu seq2seq\n",
    "- wyciągnąć z modelu reprezentację dokumentów i porównać je ze sobą\n",
    "- wyciągnąć z modelu wagi atencji i odnaleźć dla każdego ogłoszenia najważniejsze słowa\n",
    "- sprawdzić które słowa są najbardziej znaczące dla każdej z naszych kategorii"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pracujemy na zbiorze z treściami ofert pracy. Oprócz treści oferty pracy, udostępniona jest również nazwa danego stanowiska (nie będziemy jej jednak wykorzystywać podczas tych ćwiczeń). Model został wytrenowany na zbiorze treningowym (17000 wpisów) i zwalidowany (3000 wpisów). Zbiór testowy, na którym będziemy pracować zawiera 1000 wpisów. <br>\n",
    "W każdym ze wspomnianych zbiorów znajduje się 35 kategorii, które mogą przyjąć oferty. Niektóre z nich są do siebie dość podobne (np. \"IT - Rozwój oprogramowania\" i \"IT - Administracja\", a niektóre dość różne np. \"Marketing\" i \"Inżynieria\"). Skrypty użyte do przetwarzania danych i wytrenowania ich dostępne są w pliku `Modul2 [seq2seqAttn] Przetwarzanie danych i trenowanie modelu`.\n",
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Najpierw zaimportujmy potrzebne biblioteki i zdefiniujmy istotne zmienne i funkcje."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import pickle\n",
    "import numpy as np\n",
    "from keras.models import load_model, Model\n",
    "from keras.layers import Dense, Dropout, Embedding, LSTM, Bidirectional, Input, Concatenate\n",
    "from keras.initializers import Constant\n",
    "from layers.attention import AttentionDecoder \n",
    "from utils.text_helpers import *\n",
    "from utils.analysis_helpers import *\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#wczytanie danych\n",
    "\n",
    "#data_focus - na której kolumnie się skupiamy; job_content - treść oferty; job_name - nazwa stanowiska\n",
    "#max_number_words - jaka jest wielkość naszego słownika?\n",
    "#max_seq_len - ile maksymalnie tokenów z każdej oferty pracy rozważamy? \n",
    "\n",
    "dataset_details = {\n",
    "    'data_focus': 'job_content',\n",
    "    'max_number_words': 20000,\n",
    "    'max_seq_len': 250\n",
    "}\n",
    "data = Data(dataset_details['data_focus'])\n",
    "data.preprocess_data(max_number_words=dataset_details['max_number_words'], max_seq_len=dataset_details['max_seq_len'])\n",
    "\n",
    "\n",
    "#definicja argumentów sieci\n",
    "#embedding_dim - wymiarowość każdego embeddingu\n",
    "#embeddings - klucz zawierający macierz embeddingów dla naszego słownika\n",
    "#train_embeddings - czy trenować embeddingi\n",
    "#latent_dim - wymiar ukrytych stanów w LSTM/AttentionDecoder\n",
    "#return_probabilities - klucz określający czy jesteśmy zainteresowani predykcjami (False) czy wagami atencji (True)\n",
    "#num_decoder_tokens - liczba kategorii + 2 (dekodujemy kategorię + znak, że predykcja jest zakończona)\n",
    "args = {\n",
    "    'embedding_dim': 300,\n",
    "    'embeddings': generate_embedding_matrix(load_word_embeddings('./data/wiki.pl.vec'), data.tokenizer.word_index),\n",
    "    'train_embeddings': True,\n",
    "    'latent_dim': dataset_details['max_seq_len'],\n",
    "    'dropout': 0.5, \n",
    "    'num_decoder_tokens': 37,\n",
    "    'batch_size': 500, \n",
    "    'num_epochs': 5,\n",
    "    'return_probabilities': False,\n",
    "}\n",
    "\n",
    "network = Network(args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['data_focus', 'training_set', 'test_set', 'raw_train_X', 'raw_val_X', 'raw_train_y', 'raw_val_y', 'raw_test_X', 'raw_test_y', 'tokenizer', 'cleaner', 'unique_class_names', 'train_X', 'val_X', 'test_X', 'train_y', 'train_y_input', 'train_y_output', 'val_y', 'val_y_input', 'val_y_output', 'test_y', 'test_y_input', 'test_y_output'])\n"
     ]
    }
   ],
   "source": [
    "#dzięki tej komendzie łatwo zobaczymy jakie własności zawiera nasz obiekt\n",
    "print(data.__dict__.keys())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***Zadanie 1***. Na podstawie części teoretycznej i notebooka `Modul2 [seq2seqAttn] Przetwarzanie danych i trenowanie modelu`, wypełnij strukturę sieci, która odpowiada za encoder. W ramach wykonania zadania, do sieci powinny być dodane dwa bloki - jeden odpowiadający za embeddingi inputu, a drugi za dwukierunkowy LSTM jako encoder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def seq2seq(network):\n",
    "    encoder_inputs = Input(shape=(None,), name='encoder_input')\n",
    "    #dodaj warstwę embeddingów i encodera, a następnie wczytaj model z pliku\n",
    "    embedding_layer = # name=\"embeddings\"\n",
    "    embedded_sequences = #\n",
    "    encoder = # name=\"encoder\"\n",
    "    states =  #\n",
    "    outputs_true = Input(shape=(None, None,), dtype='int64', name='decoder_input')\n",
    "    decoder_outputs = AttentionDecoder(network.latent_dim*2, network.num_decoder_tokens, \\\n",
    "        return_probabilities=network.return_probabilities, name='attention')([states, outputs_true], \\\n",
    "        use_teacher_forcing=False)\n",
    "    model = Model([encoder_inputs, outputs_true], decoder_outputs)\n",
    "    return model\n",
    "\n",
    "def load(path, network, weights=True):\n",
    "    if weights: \n",
    "        model = seq2seq(network)\n",
    "        print('model created')\n",
    "        model.load_weights(path, by_name=True)\n",
    "    else:\n",
    "        model = load_model(path, custom_objects={\"AttentionDecoder\": AttentionDecoder})\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Po pomyślnym wykonaniu zadania 1, wczytanie modelu powinno być bezproblemowe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = load('./data/model_seq2seq_job_content.h5',network, weights=False)\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Jako że mamy już wczytane dane oraz model, możemy stworzyć kolejny moduł do generowania predykcji. Predykcje w seq2seq różnią się nieco od tych w standardowych problemach klasyfikacyjnych. Nie przewidujemy pojedynczej klasy, tylko sekwencję (dla `n` kroków w czasie). <br><br>\n",
    "***Zadanie 2*** Uzupełnij moduł Predictions o funkcję `get_predictions`. Jej celem jest wygenerowanie predykcji na podanych inputach, a następnie przetworzenie jej w ten sposób, by dla każdego kroku w czasie otrzymać przewidzianą dla niego klasę.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Predictions:\n",
    "    def __init__(self, model, inputs, class_names):\n",
    "        self.model = model\n",
    "        self.inputs = inputs\n",
    "        self.class_names = class_names\n",
    "        self.raw_predictions = self.get_predictions(self.model, self.inputs, self.class_names)\n",
    "        self.predicted_labels = self.transform_predictions(self.raw_predictions, self.class_names)\n",
    "        self.true_labels = data.raw_test_y\n",
    "        \n",
    "    def transform_predictions(self, predictions, class_names):\n",
    "        #funkcja usuwająca predykcje z tokenami odnoszącymi się do końca sekwencji \n",
    "        x = [[class_names[item] for item in sublist] for sublist in predictions]\n",
    "        x = [a[:a.index('\\n')] if '\\n' in a else a for a in x ]\n",
    "        return x\n",
    "\n",
    "    def get_predictions(self, model, inputs, class_names):\n",
    "        #uzupełnij funkcję\n",
    "        \n",
    "        return predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = Predictions(model, [data.test_X, data.test_y_input], data.unique_class_names)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sprawdźmy skuteczność naszego modelu na zbiorze testowym (0.93 na zbiorze walidacyjnym)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "correct_values = [i for i,x in enumerate(predictions.predicted_labels) if x[0]==data.raw_test_y.values[i]]\n",
    "print(len(correct_values)/len(predictions.predicted_labels))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Model radzi sobie całkiem nieźle. Ale jeszcze wiele możemy zrobić, żeby poznać mechanizmy jego działania. Sprawdźmy które klasy najczęściej są ze sobą mylone - `confusion matrix`.\n",
    "\n",
    "**Zadanie 3** Zaimplementuj confusion matrix. \n",
    "- Zredukuj liczbę klas, żeby nie brać pod uwagę tokenów otwierających i kończących predykcję\n",
    "- użyj funkcji `create_matrix_from_labels` z pliku `utils.analysis_helpers` do stworzenia matrixowej reprezentacji prawdziwych i przewidzianych kategorii(osobno)\n",
    "- użyj funkcji `cooccurrence_matrix` z pliku `utils.analysis_helpers` do stworzenia confusion matrix (jaki jest input do tej funkcji?) \n",
    "- użyj funkcji `plot_matrix` z pliku `utils.analysis_helpers` do pokazania confusion matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reduced_class_names = #\n",
    "true_matrix = #\n",
    "pred_matrix = #\n",
    "cooc_matrix = #"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nasz model jest przydatny nie tylko do skutecznego przewidywania kategorii, ale może być również wykorzystany w wyszukiwaniu podobnych ofert pracy. <br> \n",
    "W ramach tego ćwiczenia utniemy sieć przed dekoderem i dla każdego wpisu otrzymamy jego reprezentację w przestrzeni wektorowej (po przejściu przez warstwę Embedding i Bidirectional(LSTM)). Następnie wybierzemy kilka ofert, dla których znajdziemy inne, najbardziej do nich podobne oferty. \n",
    "\n",
    "***Zadanie 4*** \n",
    "Wydziel z modelu reprezentację wektorową dokumentów. Aby to zrobić: \n",
    "- zidentyfikuj co jest inputem i outputem naszej \"zredukowanej sieci\" (zapisz nazwy warstw)\n",
    "- stwórz kolejny, zredukowany model\n",
    "- wygeneruj na nim predykcje\n",
    "- sprawdź kształt wyniku. Coś nie pasuje, prawda?\n",
    "- pozbądź się środkowego wymiaru poprzez uśrednienie. Każdy dokument powinien być reprezentowany poprzez jedną liczbę dla jednego timestepu (zauważ, że prawdopodobnie ostatni wymiar to nasze latent_dim*2 (mamy do czynienia z Bidirectional LSTM)\n",
    "- ponownie sprawdź kształt wyniku\n",
    "- **4a** Wygeneruj i przedstaw za pomocą wykresu reprezentację 2D naszych dokumentów.\n",
    "- **4b** Porównaj ze sobą graficznie miejsca reprezentacji dwóch podobnych kategorii (np. IT - Rozwój oprogramowania i IT Administracja, albo Sprzedaż i Obsługa klienta)\n",
    "- **4c** Porównaj ze sobą graficznie miejsca reprezentacji dwóch różnych kategorii (np. Marketing i Inżynieria)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_names = [x.name for x in model.layers]\n",
    "input_reduced = #jaki jest nasz input\n",
    "output_reduced = #jaki jest nasz output\n",
    "model_reduced = Model(inputs=input_reduced, outputs=output_reduced)\n",
    "vector_representations = model_reduced.predict(data.test_X)\n",
    "\n",
    "#uśrednienie \n",
    "vector_representations_avgd = #"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#4a\n",
    "from sklearn.manifold import TSNE\n",
    "\n",
    "tsne= TSNE(n_components=2, random_state=42, verbose=2, perplexity=120)\n",
    "reduced_vector_representations = tsne.fit_transform(vector_representations)\n",
    "\n",
    "colors_categories = {x: tuple(np.random.rand(3,1).reshape(-1)) for x in data.unique_class_names}\n",
    "main_category = [tuple(colors_categories[x]) for x in data.raw_test_y]\n",
    "\n",
    "plt.figure(figsize=(15,10))\n",
    "plt.scatter(reduced_vector_representations[:,0], reduced_vector_representations[:,1], color=main_category)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#4a - zrób to samo, tylko że dla przewidzianej kategorii (nie prawdziwej)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#4b i 4c\n",
    "cat_a = # kategoria 1\n",
    "cat_b = #kategoria 2\n",
    "similar_categories_ind = # uzupełnij\n",
    "plt.scatter(reduced_vector_representations[similar_categories_ind,0], reduced_vector_representations[similar_categories_ind, 1], color=np.array(main_category)[similar_categories_ind], alpha=0.6)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**4.4** Oprócz reprezentacji wizualnej, wyszukamy najbardziej podobne treści ofert pracy za pomocą KNN (k-nearest neighbors). Napisz funkcję, która będzie wyświetlać n najbliższych sąsiadów danego wpisu. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neighbors import NearestNeighbors\n",
    "\n",
    "#fitujemy dla najbliższych 5 sąsiadów\n",
    "nnb = NearestNeighbors(n_neighbors=5)\n",
    "nnb.fit(vector_representations)\n",
    "\n",
    "def get_nearest_offers(idx, data):\n",
    "    vec = vector_representations[idx]\n",
    "    distances, indices = nnb.kneighbors(vec)\n",
    "    similar = []\n",
    "    #uzupełnij i posortuj po odległości\n",
    "    return similar"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## W końcu atencja...\n",
    "Kolejne dwa zadania będą wykorzystywać wyniki uzyskane dzięki mechanizmowi atencji. Jako że w tym zadaniu operujemy na przewidywanej sekwencji składającej się z jednego elementu, a także treści ofert pracy charakteryzują się znaczną długością, wizualizacja atencji tak jak jest dostępna w typowych publikacjach z tego zakresu (NMT) jest ciężka do osiągnięcia. Mimo tego, możemy sprawdzić, które słowa zostały zidentyfikowane jako najważniejsze dla każdej oferty pracy. \n",
    "<br>\n",
    "<br>\n",
    "Najpierw wczytamy predykcje z wagami atencji. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "network_probabilities = Network(args)\n",
    "network_probabilities.return_probabilities = True\n",
    "model_proba = load('./data/model_seq2seq_job_content.h5', network_probabilities)\n",
    "probabilities = model_proba.predict([data.test_X, data.test_y_input])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Poeksploruj kształt outputu z probabilities. Co Ci mówi? <br>\n",
    "***Zadanie 5*** Napisz funkcję, która dla dowolnej predykcji  będzie zwracała top_n najważniejszych słów wraz z wartością ich wag. \n",
    "- input: indeks oferty pracy\n",
    "- najpierw znajdź dla niej predykcję\n",
    "- posortuj po najważniejszych argumentach\n",
    "- dla każdego z najważniejszych argumentów odnajdź jakie to słowo i jaka jest jego waga\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def display_most_significant_words(index, data, probabilities, predictions, top_n=10):\n",
    "    #index - indeks wpisu\n",
    "    #data - zmienna obiektu Data\n",
    "    #probabilities - wygenerowane wagi atencji\n",
    "    #predictions - moduł predykcji\n",
    "    return \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display_most_significant_words(300, data, probabilities, predictions.predicted_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.preprocessing.text import text_to_word_sequence\n",
    "\n",
    "#dodatkowy kod do graficznej wizualizacji atencji\n",
    "\n",
    "def sequence2txt(data, ind, tokenizer):\n",
    "    return text_to_word_sequence(tokenizer.sequences_to_texts([data[ind]])[0])\n",
    "\n",
    "def visualize_attention(index, network, data, probabilities, predictions, predicted=True):\n",
    "    print(\"Real labels: {}\".format(data.test_y.values[index][1:-1]))\n",
    "    plt.clf()\n",
    "    f = plt.figure(figsize=(8, 8.5))\n",
    "    ax = f.add_subplot(1, 1, 1)\n",
    "    # add image\n",
    "    if predicted:\n",
    "        output = predictions\n",
    "    else:\n",
    "        output = data.test_y.values\n",
    "    i = ax.imshow(np.squeeze(probabilities[index][:len(output[index]),:len(sequence2txt(data.test_X, index, data.tokenizer))], axis=-1), interpolation='nearest')\n",
    "\n",
    "    # add colorbar\n",
    "    cbaxes = f.add_axes([0.2, 0.1, 0.6, 0.04])\n",
    "    cbar = f.colorbar(i, cax=cbaxes, orientation='horizontal')\n",
    "    cbar.ax.set_xlabel('Probability', labelpad=2)\n",
    "\n",
    "    # add labels\n",
    "    ax.set_yticks(range(network.max_labels)[:len(output[index])])\n",
    "    ax.set_yticklabels(output[index])\n",
    "\n",
    "    ax.set_xticks(range(network.latent_dim)[:len(sequence2txt(data.test_X, index, data.tokenizer))])\n",
    "    ax.set_xticklabels(sequence2txt(data.test_X, index, data.tokenizer), rotation=45)\n",
    "\n",
    "    ax.set_xlabel('Input text')\n",
    "    ax.set_ylabel('Category')\n",
    "    ax.grid()\n",
    "\n",
    "    f.show()\n",
    "    \n",
    "visualize_attention(np.argmin([len(x.split(\" \")) for x in data.raw_test_X.values]), network, data, probabilities, predictions.predicted_labels)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***Zadanie 6*** Napisz funkcję, która zbierze najważniejsze słowa dla wpisów należących do danej kategorii (przewidzianych). Funkcja powinna zwracać słownik, w której kluczami są słowa, a wartościami jest lista wartości odpowiadających wagom.\n",
    "- znajdź odpowiednie id ofert z danej kategorii\n",
    "- dla każdej z tych ofert zbierz najważniejsze dla niej słowa i ich wagi \n",
    "- zapisz każdy wynik w słowniku, w którym klucze to słowa, a wartości to lista wag atencji dla poszczególnych słów"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_most_significant_word_category(category, data, probabilities, predictions, top_n=10):\n",
    "    relevant_offers_ids = #znajdź odpowiednie id ofert \n",
    "    relevant_probabilities_args_sorted = #posortowane wg ważności indeksy słów dla danych ofert\n",
    "    words = {}\n",
    "    \n",
    "    for i,offer in enumerate(data.test_X[relevant_offers_ids]):\n",
    "        curr_words = #jakie są słowa które są najważniejsze\n",
    "        for j,x in enumerate(curr_words):\n",
    "            #sprawdź czy to słowo jest już w naszym słowniku, jeśli tak, to dodaj tę wagę, jeśli nie, to stwórz pustą listę               \n",
    "    return words  \n",
    "\n",
    "\n",
    "def prepare_for_wordcloud_format(*args):\n",
    "    summed_words_importance_dict = {}\n",
    "    for dictionary in args:\n",
    "        for key in dictionary.keys():\n",
    "            summed_words_importance_dict[key] = summed_words_importance_dict.get(key, 0)\n",
    "            summed_words_importance_dict[key] += int(np.ceil(10*sum(dictionary[key])))\n",
    "    wordcloud_format_list = [(x[0] + \" \")*x[1] for x in sorted(summed_words_importance_dict.items(), key=lambda x: x[1], reverse=True)]\n",
    "    wordcloud_format_string = \"\".join(wordcloud_format_list)\n",
    "    return wordcloud_format_string\n",
    "\n",
    "def generate_wordcloud(wordcloud_string):\n",
    "    wc = WordCloud(collocations=False, width=800, height=500, background_color='#eee8ef', colormap='nipy_spectral').generate(wordcloud_string)\n",
    "    fig = plt.figure(figsize=(30,20))\n",
    "    img = plt.imshow(wc, aspect='auto')\n",
    "    plt.axis('off')\n",
    "    plt.show()\n",
    "    return \n",
    "\n",
    "\n",
    "generate_wordcloud(prepare_for_wordcloud_format(get_most_significant_word_category(\"Sprzedaż\", data, probabilities, predictions.predictions)))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
